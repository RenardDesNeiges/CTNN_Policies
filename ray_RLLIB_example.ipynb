{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for supervised training of LTC cells for time-series prediction tasks\n",
    "#### *(using the Pytorch implementation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a config dictionary for our trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 8,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent with our defined config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 20:17:57,630\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[2m\u001b[36m(pid=50226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=50223)\u001b[0m \n",
      "2022-03-13 20:18:04,184\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-03-13 20:18:04,185\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-03-13 20:18:04,224\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=50226)\u001b[0m 2022-03-13 20:18:04,168\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=50223)\u001b[0m 2022-03-13 20:18:04,168\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0 , \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=50221)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len : 22.223463687150836\n",
      "avg_rev : 22.223463687150836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=50221)\u001b[0m 2022-03-13 20:18:09,423\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1 , \n",
      "len : 29.00735294117647\n",
      "avg_rev : 29.00735294117647\n",
      "iteration : 2 , \n",
      "len : 47.11\n",
      "avg_rev : 47.11\n",
      "iteration : 3 , \n",
      "len : 78.14\n",
      "avg_rev : 78.14\n",
      "iteration : 4 , \n",
      "len : 104.68\n",
      "avg_rev : 104.68\n",
      "iteration : 5 , \n",
      "len : 129.69\n",
      "avg_rev : 129.69\n",
      "iteration : 6 , \n",
      "len : 164.62\n",
      "avg_rev : 164.62\n",
      "iteration : 7 , \n",
      "len : 194.26\n",
      "avg_rev : 194.26\n",
      "iteration : 8 , \n",
      "len : 219.31\n",
      "avg_rev : 219.31\n",
      "iteration : 9 , \n",
      "len : 251.01\n",
      "avg_rev : 251.01\n",
      "iteration : 10 , \n",
      "len : 278.5\n",
      "avg_rev : 278.5\n",
      "iteration : 11 , \n",
      "len : 305.15\n",
      "avg_rev : 305.15\n",
      "iteration : 12 , \n",
      "len : 329.61\n",
      "avg_rev : 329.61\n",
      "iteration : 13 , \n",
      "len : 334.93\n",
      "avg_rev : 334.93\n",
      "iteration : 14 , \n",
      "len : 350.22\n",
      "avg_rev : 350.22\n",
      "iteration : 15 , \n",
      "len : 342.34\n",
      "avg_rev : 342.34\n",
      "iteration : 16 , \n",
      "len : 322.29\n",
      "avg_rev : 322.29\n",
      "iteration : 17 , \n",
      "len : 318.71\n",
      "avg_rev : 318.71\n",
      "iteration : 18 , \n",
      "len : 302.14\n",
      "avg_rev : 302.14\n",
      "iteration : 19 , \n",
      "len : 296.95\n",
      "avg_rev : 296.95\n"
     ]
    }
   ],
   "source": [
    "# Instanciate the PPO trainer object\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "log = []\n",
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    print(\"iteration : \" +str(i), \", \")\n",
    "    log.append(trainer.train())\n",
    "    print('len : ' + str(log[i]['episode_len_mean']))\n",
    "    print('avg_rev : ' + str(np.array(log[i]['hist_stats']['episode_reward']).mean()))\n",
    "    if i % 5 == 0:\n",
    "        trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 500.0,\n",
       "  'episode_reward_min': 176.0,\n",
       "  'episode_reward_mean': 344.2,\n",
       "  'episode_len_mean': 344.2,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [225.0,\n",
       "    278.0,\n",
       "    294.0,\n",
       "    500.0,\n",
       "    423.0,\n",
       "    316.0,\n",
       "    500.0,\n",
       "    441.0,\n",
       "    176.0,\n",
       "    289.0],\n",
       "   'episode_lengths': [225, 278, 294, 500, 423, 316, 500, 441, 176, 289]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.1307517243951104,\n",
       "   'mean_inference_ms': 1.1028653226296545,\n",
       "   'mean_action_processing_ms': 0.06369292459067953,\n",
       "   'mean_env_wait_ms': 0.10128286618736895,\n",
       "   'mean_env_render_ms': 7.619497852004665},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8fdc6dd3a09dc5c83aad34f80e2b308d8dfb410ffac43971d3f03fb3420545d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torchNCP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
