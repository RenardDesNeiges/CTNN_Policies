{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook training of an RL agent using Ray RLlib\n",
    "#### *(using th CartPole-v1 gym environnment)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:45652',\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-14_14-14-37_049828_59049/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-14_14-14-37_049828_59049/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-03-14_14-14-37_049828_59049',\n",
       " 'metrics_export_port': 62205,\n",
       " 'gcs_address': '127.0.0.1:57162',\n",
       " 'node_id': '0f3300e6b52dfa46dcb8206bd8f8470a3716f79a5dfc79137fc9a150'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import LTCRL.utils as lru               # Utilities for training LTCs with pytorch\n",
    "import LTCRL.models as models           # RayRLlib model implementations\n",
    "from ray.rllib.examples.models.rnn_model import TorchRNNModel\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a config dictionary for our trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"naiveRNN\", models.NaiveRNN)\n",
    "\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 1,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"naiveRNN\",\n",
    "        \"max_seq_len\": 20,\n",
    "        \"custom_model_config\": {\n",
    "            \"cell_size\": 32,\n",
    "        },\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent with our defined config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 14:13:47,994\tWARNING trainer.py:2279 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[2m\u001b[36m(pid=59007)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m 2022-03-14 14:13:53,669\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m 2022-03-14 14:13:53,682\tERROR worker.py:432 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=59007, ip=127.0.0.1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 588, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1555, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self[policy_id] = class_(observation_space, action_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self._initialize_loss_from_dummy_batch()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy.py\", line 801, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     return self._compute_action_helper(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     dist_inputs, state_out = self.model(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/torch/recurrent_net.py\", line 83, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     output, new_state = self.forward_rnn(inputs, state, seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/Documents/etudes/EPFLMA4/LTC/code/LTCRL/models.py\", line 65, in forward_rnn\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self._features, [h, c] = self.rnn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     return forward_call(*input, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 265, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self.check_forward_args(input, hx, batch_sizes)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 232, in check_forward_args\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     self.check_hidden_size(hidden, expected_hidden_size)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m   File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 226, in check_hidden_size\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m     raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59007)\u001b[0m RuntimeError: Expected hidden size (1, 32, 64), got [1, 32]\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=59007, ip=127.0.0.1)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 588, in __init__\n    self._build_policy_map(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1555, in _build_policy_map\n    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n    self[policy_id] = class_(observation_space, action_space,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n    self._initialize_loss_from_dummy_batch()\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy.py\", line 801, in _initialize_loss_from_dummy_batch\n    self.compute_actions_from_input_dict(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n    return self._compute_action_helper(input_dict, state_batches,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n    dist_inputs, state_out = self.model(input_dict, state_batches,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/torch/recurrent_net.py\", line 83, in forward\n    output, new_state = self.forward_rnn(inputs, state, seq_lens)\n  File \"/Users/renard/Documents/etudes/EPFLMA4/LTC/code/LTCRL/models.py\", line 65, in forward_rnn\n    self._features, [h, c] = self.rnn(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 265, in forward\n    self.check_forward_args(input, hx, batch_sizes)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 232, in check_forward_args\n    self.check_hidden_size(hidden, expected_hidden_size)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 226, in check_hidden_size\n    raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\nRuntimeError: Expected hidden size (1, 32, 64), got [1, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=809'>810</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=810'>811</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=811'>812</a>\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=812'>813</a>\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=813'>814</a>\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=816'>817</a>\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=817'>818</a>\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:923\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=920'>921</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict,\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=921'>922</a>\u001b[0m           env_creator: Callable[[EnvContext], EnvType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=922'>923</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39m# Instanciate the PPO trainer object\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m PPOTrainer(config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=3'>4</a>\u001b[0m \u001b[39m# Run it for n training iterations. A training iteration includes\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39m# parallel sample collection by the environment workers as well as\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39m# loss calculation on the collected batch and a model update.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000006?line=6'>7</a>\u001b[0m log \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:728\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=724'>725</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_workers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=725'>726</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=727'>728</a>\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, logger_creator, remote_checkpoint_dir,\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=728'>729</a>\u001b[0m                  sync_function_tpl)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py:122\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py?line=119'>120</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py?line=122'>123</a>\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/tune/trainable.py?line=123'>124</a>\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:826\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=811'>812</a>\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=812'>813</a>\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=813'>814</a>\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=816'>817</a>\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=817'>818</a>\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=818'>819</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=819'>820</a>\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=820'>821</a>\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=823'>824</a>\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=824'>825</a>\u001b[0m     \u001b[39m# should no longer be used.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=825'>826</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_workers(\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=826'>827</a>\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=827'>828</a>\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=828'>829</a>\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=829'>830</a>\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=830'>831</a>\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=832'>833</a>\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=833'>834</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39m_disable_execution_plan_api\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=834'>835</a>\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=835'>836</a>\u001b[0m         \u001b[39m# local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1925\u001b[0m, in \u001b[0;36mTrainer._make_workers\u001b[0;34m(self, env_creator, validate_env, policy_class, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1888'>1889</a>\u001b[0m \u001b[39m@DeveloperAPI\u001b[39m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1889'>1890</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_workers\u001b[39m(\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1890'>1891</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1897'>1898</a>\u001b[0m         local_worker: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1898'>1899</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m WorkerSet:\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1899'>1900</a>\u001b[0m     \u001b[39m\"\"\"Default factory method for a WorkerSet running under this Trainer.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1900'>1901</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1901'>1902</a>\u001b[0m \u001b[39m    Override this method by passing a custom `make_workers` into\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1922'>1923</a>\u001b[0m \u001b[39m        The created WorkerSet.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1923'>1924</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1924'>1925</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m WorkerSet(\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1925'>1926</a>\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1926'>1927</a>\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1927'>1928</a>\u001b[0m         policy_class\u001b[39m=\u001b[39;49mpolicy_class,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1928'>1929</a>\u001b[0m         trainer_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1929'>1930</a>\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1930'>1931</a>\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1931'>1932</a>\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1932'>1933</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:100\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=91'>92</a>\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=92'>93</a>\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=93'>94</a>\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=94'>95</a>\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=95'>96</a>\u001b[0m \u001b[39mif\u001b[39;00m local_worker \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=96'>97</a>\u001b[0m         \u001b[39mnot\u001b[39;00m trainer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcreate_env_on_driver\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=97'>98</a>\u001b[0m         (\u001b[39mnot\u001b[39;00m trainer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mobservation_space\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=98'>99</a>\u001b[0m          \u001b[39mnot\u001b[39;00m trainer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39maction_space\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=99'>100</a>\u001b[0m     remote_spaces \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremote_workers(\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=100'>101</a>\u001b[0m     )[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mforeach_policy\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=101'>102</a>\u001b[0m         \u001b[39mlambda\u001b[39;49;00m p, pid: (pid, p\u001b[39m.\u001b[39;49mobservation_space, p\u001b[39m.\u001b[39;49maction_space)))\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=102'>103</a>\u001b[0m     spaces \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=103'>104</a>\u001b[0m         e[\u001b[39m0\u001b[39m]: (\u001b[39mgetattr\u001b[39m(e[\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39moriginal_space\u001b[39m\u001b[39m\"\u001b[39m, e[\u001b[39m1\u001b[39m]), e[\u001b[39m2\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=104'>105</a>\u001b[0m         \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m remote_spaces\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=105'>106</a>\u001b[0m     }\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py?line=106'>107</a>\u001b[0m     \u001b[39m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=102'>103</a>\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=103'>104</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=104'>105</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py:1735\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1732'>1733</a>\u001b[0m             \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1733'>1734</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1734'>1735</a>\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1736'>1737</a>\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1737'>1738</a>\u001b[0m     values \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=59007, ip=127.0.0.1)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 588, in __init__\n    self._build_policy_map(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1555, in _build_policy_map\n    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n    self[policy_id] = class_(observation_space, action_space,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n    self._initialize_loss_from_dummy_batch()\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/policy.py\", line 801, in _initialize_loss_from_dummy_batch\n    self.compute_actions_from_input_dict(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n    return self._compute_action_helper(input_dict, state_batches,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n    dist_inputs, state_out = self.model(input_dict, state_batches,\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/models/torch/recurrent_net.py\", line 83, in forward\n    output, new_state = self.forward_rnn(inputs, state, seq_lens)\n  File \"/Users/renard/Documents/etudes/EPFLMA4/LTC/code/LTCRL/models.py\", line 65, in forward_rnn\n    self._features, [h, c] = self.rnn(\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 265, in forward\n    self.check_forward_args(input, hx, batch_sizes)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 232, in check_forward_args\n    self.check_hidden_size(hidden, expected_hidden_size)\n  File \"/Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 226, in check_hidden_size\n    raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\nRuntimeError: Expected hidden size (1, 32, 64), got [1, 32]"
     ]
    }
   ],
   "source": [
    "# Instanciate the PPO trainer object\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "log = []\n",
    "iterations = 1\n",
    "for i in range(iterations):\n",
    "    print(\"iteration : \" +str(i), \", \")\n",
    "    log.append(trainer.train())\n",
    "    print('len : ' + str(log[i]['episode_len_mean']))\n",
    "    print('avg_rev : ' + str(np.array(log[i]['hist_stats']['episode_reward']).mean()))\n",
    "    # if i % 5 == 0:\n",
    "        # trainer.evaluate()\n",
    "        # trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000007?line=0'>1</a>\u001b[0m mean_rev \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(l[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m log))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000007?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(mean_rev)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/renard/Documents/etudes/EPFLMA4/LTC/code/ray_RLLIB_example_acrobot.ipynb#ch0000007?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mMean reward (reward = ep_length)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "mean_rev = np.array(list(l['episode_reward_mean'] for l in log))\n",
    "plt.plot(mean_rev)\n",
    "plt.title('Mean reward (reward = ep_length)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Evaluate the trained Trainer (and render each timestep to the shell's\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# output).\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1218\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, episodes_left_fn, duration_fn)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1214'>1215</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1216'>1217</a>\u001b[0m round_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1217'>1218</a>\u001b[0m batches \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget([\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1218'>1219</a>\u001b[0m     w\u001b[39m.\u001b[39;49msample\u001b[39m.\u001b[39;49mremote() \u001b[39mfor\u001b[39;49;00m i, w \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1219'>1220</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation_workers\u001b[39m.\u001b[39;49mremote_workers())\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1220'>1221</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m i \u001b[39m*\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39mif\u001b[39;49;00m unit \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mepisodes\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39melse\u001b[39;49;00m rollout \u001b[39m*\u001b[39;49m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1221'>1222</a>\u001b[0m             num_envs) \u001b[39m<\u001b[39;49m units_left_to_do\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1222'>1223</a>\u001b[0m ])\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1223'>1224</a>\u001b[0m \u001b[39m# 1 episode per returned batch.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/rllib/agents/trainer.py?line=1224'>1225</a>\u001b[0m \u001b[39mif\u001b[39;00m unit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepisodes\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=102'>103</a>\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=103'>104</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/_private/client_mode_hook.py?line=104'>105</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py:1726\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1721'>1722</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject_refs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must either be an object ref \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1722'>1723</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mor a list of object refs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1724'>1725</a>\u001b[0m \u001b[39m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1725'>1726</a>\u001b[0m values, debugger_breakpoint \u001b[39m=\u001b[39m worker\u001b[39m.\u001b[39;49mget_objects(\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1726'>1727</a>\u001b[0m     object_refs, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1727'>1728</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, value \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(values):\n\u001b[1;32m   <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=1728'>1729</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py:354\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=348'>349</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=349'>350</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to call `get` on the value \u001b[39m\u001b[39m{\u001b[39;00mobject_ref\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=350'>351</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhich is not an ray.ObjectRef.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=352'>353</a>\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m) \u001b[39mif\u001b[39;00m timeout \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=353'>354</a>\u001b[0m data_metadata_pairs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcore_worker\u001b[39m.\u001b[39;49mget_objects(\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=354'>355</a>\u001b[0m     object_refs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_task_id, timeout_ms)\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=355'>356</a>\u001b[0m debugger_breakpoint \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/renard/miniconda3/envs/torchLTC/lib/python3.8/site-packages/ray/worker.py?line=356'>357</a>\u001b[0m \u001b[39mfor\u001b[39;00m (data, metadata) \u001b[39min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1180\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:165\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8fdc6dd3a09dc5c83aad34f80e2b308d8dfb410ffac43971d3f03fb3420545d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('torchNCP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
